{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling elections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The `electoral_votes` variable is a dictionary containing the number of Electoral College votes for each state. For example\n",
    "```\n",
    "  >>> electoral_votes['Indiana']\n",
    "  11\n",
    "```\n",
    "Data from [Wikipedia: United_States_Electoral_College](https://en.wikipedia.org/wiki/United_States_Electoral_College)\n",
    "\n",
    "The `survey_results` variable is a dictionary mapping from states to an array of survey results for each candidate. **Each row in a survey results array represents one survey** and each column represents one candidate. There are **4 columns, representing Clinton, Trump, Johnson, and Stein** in that order. In the example below, Clinton got 340 votes in the first survey, Trump got 258, Johnson got 27, and Stein got 13.\n",
    "```\n",
    "  >>> survey_results['Indiana']\n",
    "  array([[340, 258,  27,  13],\n",
    "         [240, 155,   5,   5],\n",
    "         [235, 155,  50,  20],\n",
    "         [308, 266,  49,  35],\n",
    "         [222, 161,  80,  30]])\n",
    "```\n",
    "Data from [Wikipedia: Statewide opinion polling for the United States presidential election, 2016](https://en.wikipedia.org/wiki/Statewide_opinion_polling_for_the_United_States_presidential_election,_2016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeling 5 states with 38 electoral college votes\n"
     ]
    }
   ],
   "source": [
    "electoral_votes = {\n",
    "    'Alabama': 9,\n",
    "    'Alaska': 3,\n",
    "    'Arizona': 11,\n",
    "    'Arkansas': 6,\n",
    "    'Colorado': 9,\n",
    "}\n",
    "\n",
    "survey_results = {\n",
    "    'Alabama': np.array([], dtype=int).reshape(0, 4),\n",
    "    'Alaska': np.array([400 * np.array([.47, .43, .07, .03]), 500 * np.array([.36, .37, .07, .03]), 500 * np.array([.34, .37, .10, .02]), 660 * np.array([.31, .36, .18, .06])], dtype=int),\n",
    "    'Arizona': np.array([392 * np.array([.45, .47, .05, .02]), 550 * np.array([.39, .47, .04, .03]), 719 * np.array([.40, .45, .09, .03]), 769 * np.array([.44, .49, .05, .01]), 2229 * np.array([.45, .44, .07, .01]), 700 * np.array([.43, .47, .02, .02]), 550 * np.array([.41, .45, .03, .01]), 994 * np.array([.42, .44, .04, .01]), 550 * np.array([.40, .42, .05, .02]), 2385 * np.array([.48, .46, .05, .01]), 401 * np.array([.45, .46, .04, .01]), 550 * np.array([.41, .41, .05, .02]), 1538 * np.array([.39, .44, .06, .02]), 713 * np.array([.43, .38, .06, .01]), 400 * np.array([.39, .37, .08, .03]), 600 * np.array([.44, .42, .09, .01]), 718 * np.array([.42, .42, .05, .01]), 484 * np.array([.41, .46, .09, .01]), 649 * np.array([.38, .40, .12, .03])], dtype=int),\n",
    "    'Arkansas': np.array([463 * np.array([.33, .56, .04, .02]), 831 * np.array([.34, .55, .03, .01]), 600 * np.array([.29, .57, .05, .03])], dtype=int),\n",
    "    'Colorado': np.array([1150 * np.array([.45, .44, .05, .04]), 500 * np.array([.44, .38, .07, .02]), 550 * np.array([.39, .39, .05, .04]), 750 * np.array([.44, .41, .08, .04]), 685 * np.array([.45, .37, .10, .03]), 400 * np.array([.49, .38, .07, .03]), 602 * np.array([.44, .33, .10, .03]), 694 * np.array([.46, .40, .06, .02]), 784 * np.array([.41, .42, .13, .03]), 991 * np.array([.40, .39, .07, .02]), 644 * np.array([.44, .42, .10, .02]), 540 * np.array([.41, .34, .12, .03]), 600 * np.array([.38, .42, .13, .02]), 704 * np.array([.48, .43, .04, .02]), 605 * np.array([.43, .38, .07, .02]), 997 * np.array([.42, .39, .07, .02])], dtype=int),\n",
    "}\n",
    "\n",
    "states = sorted(survey_results.keys())\n",
    "print('Modeling', len(states), 'states with', sum(electoral_votes[s] for s in states), 'electoral college votes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': array([], shape=(0, 4), dtype=int64), 'Alaska': array([[188, 172,  28,  12],\n",
      "       [180, 185,  35,  15],\n",
      "       [170, 185,  50,  10],\n",
      "       [204, 237, 118,  39]]), 'Arizona': array([[ 176,  184,   19,    7],\n",
      "       [ 214,  258,   22,   16],\n",
      "       [ 287,  323,   64,   21],\n",
      "       [ 338,  376,   38,    7],\n",
      "       [1003,  980,  156,   22],\n",
      "       [ 301,  329,   14,   14],\n",
      "       [ 225,  247,   16,    5],\n",
      "       [ 417,  437,   39,    9],\n",
      "       [ 220,  231,   27,   11],\n",
      "       [1144, 1097,  119,   23],\n",
      "       [ 180,  184,   16,    4],\n",
      "       [ 225,  225,   27,   11],\n",
      "       [ 599,  676,   92,   30],\n",
      "       [ 306,  270,   42,    7],\n",
      "       [ 156,  148,   32,   12],\n",
      "       [ 264,  252,   54,    6],\n",
      "       [ 301,  301,   35,    7],\n",
      "       [ 198,  222,   43,    4],\n",
      "       [ 246,  259,   77,   19]]), 'Arkansas': array([[152, 259,  18,   9],\n",
      "       [282, 457,  24,   8],\n",
      "       [174, 341,  30,  18]]), 'Colorado': array([[517, 506,  57,  46],\n",
      "       [220, 190,  35,  10],\n",
      "       [214, 214,  27,  22],\n",
      "       [330, 307,  60,  30],\n",
      "       [308, 253,  68,  20],\n",
      "       [196, 152,  28,  12],\n",
      "       [264, 198,  60,  18],\n",
      "       [319, 277,  41,  13],\n",
      "       [321, 329, 101,  23],\n",
      "       [396, 386,  69,  19],\n",
      "       [283, 270,  64,  12],\n",
      "       [221, 183,  64,  16],\n",
      "       [228, 252,  78,  12],\n",
      "       [337, 302,  28,  14],\n",
      "       [260, 229,  42,  12],\n",
      "       [418, 388,  69,  19]])}\n"
     ]
    }
   ],
   "source": [
    "print(survey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative model\n",
    "\n",
    "1. For each state we generate an $\\vec{\\alpha}$ vector, which defines a Dirichlet distribution over the proportion of votes that go to each of the 4 candidates whenever we do a survey — including the final survey, namely the election itself which we want to predict. The **prior over each component of $\\vec{\\alpha}$ is taken as a (positive half-) Cauchy distribution** with location 0 and scale 1. Since the components of $\\vec{\\alpha}$ are positive, we actually use the positive half-Cauchy distribution.\n",
    "\n",
    "2. For each survey in a state we **generate a probability vector $\\vec{p_i} \\sim \\text{Dirichlet}(\\vec{\\alpha})$ for the probability that a voter selects each of the 4 candidates**.\n",
    "\n",
    "3. For each survey, we then generate the number of votes going to each candidate **(posterior) as $\\vec{k_i} \\sim \\text{Multinomial}(\\vec{p_i})$.**\n",
    "\n",
    "### Tasks\n",
    "\n",
    "* Use Stan to sample from the posterior distribution over $\\alpha$ and visualize your results. There are 10 states, so you will have 10 posteriors.\n",
    "* The posteriors over $\\alpha$ show a lot of variation between different states. Explain the results you get in terms of the model and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the model we want to build like this:\n",
    "\n",
    "- model for alpha -> pos-half-cauchy (0,1)\n",
    "- model for p -> dirichlet(alpha)\n",
    "- posterior model k -> multinomial(p)\n",
    "\n",
    "Since the prior parameters for the Cauchy distribution are preset at 0,1, we don't need to add them to the data. All other parameters can be entered in the \"parameter\" section of our model then, since they are not predetermined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_3569e5a0cd6c3c10792bde5f98dd5a51 NOW.\n"
     ]
    }
   ],
   "source": [
    "stan_code = '''\n",
    "data {\n",
    "    int<lower=0> N;  //num of candidates\n",
    "    int<lower=0> S;  //num of surveys in a state\n",
    "    int<lower=0> polls[S, N];   //matrix with survey results   \n",
    "}\n",
    "\n",
    "parameters {\n",
    "    vector<lower=0>[N] alpha;    //dirichlet parameter\n",
    "    simplex[N] p[S];  //multinomial parameter\n",
    "}\n",
    "\n",
    "model {\n",
    "    \n",
    "    alpha ~ cauchy(0,1);\n",
    "    \n",
    "    for (j in 1:S) {\n",
    "        p[j] ~ dirichlet(alpha);\n",
    "        polls[j] ~ multinomial(p[j]);\n",
    "    }\n",
    "    \n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "stan_model = pystan.StanModel(model_code=stan_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run this model for each state if we iteratively define the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:1 of 4000 iterations ended with a divergence (0.025 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n",
      "WARNING:pystan:134 of 4000 iterations ended with a divergence (3.35 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alabama': Inference for Stan model: anon_model_3569e5a0cd6c3c10792bde5f98dd5a51.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha[1]   5.04    0.79  33.35   0.04   0.38   0.99   2.48  25.91   1797    1.0\n",
      "alpha[2]   4.31    0.47  24.42   0.03   0.37   0.94   2.35  24.67   2685    1.0\n",
      "alpha[3]   9.12    3.51 178.61   0.04   0.42   0.99   2.41  27.87   2585    1.0\n",
      "alpha[4]   8.21    2.63 162.59   0.04   0.41   0.97   2.33  25.78   3816    1.0\n",
      "lp__      -5.59    0.05   1.82 -10.22  -6.59   -5.2  -4.23  -3.17   1347    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Oct 24 16:09:39 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1)., 'Alaska': Inference for Stan model: anon_model_3569e5a0cd6c3c10792bde5f98dd5a51.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha[1]  13.32    0.32   8.86   2.98   7.25  11.28  16.87  36.26    786    1.0\n",
      "alpha[2]   13.9    0.33    9.2   3.03    7.6  11.74  17.68  38.22    785    1.0\n",
      "alpha[3]   3.71    0.08   2.42   0.84   2.03    3.1   4.74  10.07    859    1.0\n",
      "alpha[4]   1.51    0.03   0.86   0.43   0.91   1.32   1.89   3.72   1042    1.0\n",
      "p[1,1]     0.47  3.4e-4   0.02   0.42   0.45   0.47   0.48   0.51   5227    1.0\n",
      "p[2,1]     0.43  3.6e-4   0.02   0.39   0.41   0.43   0.45   0.48   4491    1.0\n",
      "p[3,1]     0.41  3.4e-4   0.02   0.36   0.39   0.41   0.42   0.46   4608    1.0\n",
      "p[4,1]     0.34  2.8e-4   0.02   0.31   0.33   0.34   0.36   0.38   4880    1.0\n",
      "p[1,2]     0.43  3.4e-4   0.02   0.38   0.41   0.43   0.45   0.48   4861    1.0\n",
      "p[2,2]     0.45  3.3e-4   0.02    0.4   0.43   0.45   0.46   0.49   5017    1.0\n",
      "p[3,2]     0.44  3.5e-4   0.02    0.4   0.43   0.44   0.46   0.49   4595    1.0\n",
      "p[4,2]      0.4  2.8e-4   0.02   0.36   0.38    0.4   0.41   0.44   4908    1.0\n",
      "p[1,3]     0.07  2.0e-4   0.01   0.05   0.06   0.07   0.08    0.1   4185    1.0\n",
      "p[2,3]     0.09  2.0e-4   0.01   0.06   0.08   0.09    0.1   0.11   4630    1.0\n",
      "p[3,3]     0.12  2.3e-4   0.02   0.09   0.11   0.12   0.13   0.15   4404    1.0\n",
      "p[4,3]     0.19  2.3e-4   0.02   0.16   0.18   0.19    0.2   0.22   4646    1.0\n",
      "p[1,4]     0.03  1.2e-4 8.4e-3   0.02   0.03   0.03   0.04   0.05   5067    1.0\n",
      "p[2,4]     0.04  1.3e-4 8.8e-3   0.02   0.03   0.04   0.04   0.06   4321    1.0\n",
      "p[3,4]     0.03  1.1e-4 7.7e-3   0.01   0.02   0.02   0.03   0.04   4572    1.0\n",
      "p[4,4]     0.06  1.5e-4   0.01   0.05   0.06   0.06   0.07   0.09   4722    1.0\n",
      "lp__      -2042    0.07   2.93  -2049  -2044  -2042  -2040  -2038   1597    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Oct 24 16:09:42 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1)., 'Arizona': Inference for Stan model: anon_model_3569e5a0cd6c3c10792bde5f98dd5a51.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha[1]   89.0    0.61  24.63  49.08  71.37  85.63 103.01 145.74   1611    1.0\n",
      "alpha[2]  92.49    0.64  25.62  51.42  74.27  88.92 107.09 150.52   1617    1.0\n",
      "alpha[3]  12.08    0.08   3.38    6.7   9.63  11.62  14.04  19.79   1637    1.0\n",
      "alpha[4]   3.51    0.02   0.96   1.97   2.83   3.39   4.06   5.82   1742    1.0\n",
      "p[1,1]     0.45  2.4e-4   0.02   0.41   0.44   0.45   0.47    0.5   7967    1.0\n",
      "p[2,1]     0.43  2.1e-4   0.02   0.39   0.42   0.43   0.44   0.47   8375    1.0\n",
      "p[3,1]     0.42  1.9e-4   0.02   0.39   0.41   0.42   0.43   0.45   8514    1.0\n",
      "p[4,1]     0.45  1.7e-4   0.02   0.42   0.44   0.45   0.46   0.48   8672    1.0\n",
      "p[5,1]     0.46  1.1e-4   0.01   0.44   0.46   0.46   0.47   0.48   9780    1.0\n",
      "p[6,1]     0.46  1.9e-4   0.02   0.42   0.44   0.46   0.47   0.49   8634    1.0\n",
      "p[7,1]     0.46  1.8e-4   0.02   0.42   0.44   0.46   0.47   0.49  11059    1.0\n",
      "p[8,1]     0.46  1.5e-4   0.02   0.43   0.45   0.46   0.47   0.49   9645    1.0\n",
      "p[9,1]     0.45  1.9e-4   0.02   0.41   0.44   0.45   0.46   0.49   9633    1.0\n",
      "p[10,1]    0.48  9.7e-510.0e-3   0.46   0.47   0.48   0.48    0.5  10648    1.0\n",
      "p[11,1]    0.46  2.1e-4   0.02   0.42   0.45   0.46   0.48    0.5  10187    1.0\n",
      "p[12,1]    0.46  2.0e-4   0.02   0.42   0.45   0.46   0.47    0.5  10228    1.0\n",
      "p[13,1]    0.43  1.4e-4   0.01   0.41   0.42   0.43   0.44   0.46   8248    1.0\n",
      "p[14,1]    0.48  2.0e-4   0.02   0.45   0.47   0.48   0.49   0.51   7656    1.0\n",
      "p[15,1]    0.45  2.2e-4   0.02   0.41   0.43   0.45   0.46   0.49   9339    1.0\n",
      "p[16,1]    0.46  2.0e-4   0.02   0.42   0.44   0.46   0.47   0.49   7877    1.0\n",
      "p[17,1]    0.46  1.9e-4   0.02   0.43   0.45   0.46   0.48    0.5   8478    1.0\n",
      "p[18,1]    0.43  2.2e-4   0.02   0.39   0.42   0.43   0.45   0.47   7989    1.0\n",
      "p[19,1]    0.42  1.9e-4   0.02   0.38   0.41   0.42   0.43   0.46   8946    1.0\n",
      "p[1,2]     0.47  2.2e-4   0.02   0.43   0.46   0.47   0.49   0.52   9131    1.0\n",
      "p[2,2]      0.5  2.1e-4   0.02   0.46   0.48    0.5   0.51   0.53   8449    1.0\n",
      "p[3,2]     0.47  1.8e-4   0.02   0.43   0.45   0.47   0.48    0.5   9192    1.0\n",
      "p[4,2]     0.49  1.8e-4   0.02   0.46   0.48   0.49    0.5   0.52   8148    1.0\n",
      "p[5,2]     0.45  1.0e-4   0.01   0.43   0.45   0.45   0.46   0.47   9998    1.0\n",
      "p[6,2]     0.49  2.0e-4   0.02   0.46   0.48   0.49   0.51   0.53   7673    1.0\n",
      "p[7,2]     0.49  1.9e-4   0.02   0.45   0.48   0.49   0.51   0.53  10419    1.0\n",
      "p[8,2]     0.48  1.5e-4   0.02   0.45   0.47   0.48   0.49   0.51  10128    1.0\n",
      "p[9,2]     0.47  1.9e-4   0.02   0.43   0.46   0.47   0.48   0.51   9601    1.0\n",
      "p[10,2]    0.46 10.0e-5   0.01   0.44   0.45   0.46   0.47   0.48  10313    1.0\n",
      "p[11,2]    0.48  2.1e-4   0.02   0.43   0.46   0.48   0.49   0.52   9912    1.0\n",
      "p[12,2]    0.46  2.1e-4   0.02   0.42   0.45   0.46   0.48    0.5   8840    1.0\n",
      "p[13,2]    0.48  1.3e-4   0.01   0.46   0.47   0.48   0.49   0.51   8781    1.0\n",
      "p[14,2]    0.44  2.0e-4   0.02   0.41   0.43   0.44   0.45   0.47   7686    1.0\n",
      "p[15,2]    0.44  2.3e-4   0.02    0.4   0.43   0.44   0.46   0.48   8892    1.0\n",
      "p[16,2]    0.45  2.0e-4   0.02   0.41   0.43   0.45   0.46   0.48   7756    1.0\n",
      "p[17,2]    0.47  1.9e-4   0.02   0.43   0.46   0.47   0.48    0.5   7925    1.0\n",
      "p[18,2]    0.47  2.1e-4   0.02   0.44   0.46   0.47   0.49   0.51   8870    1.0\n",
      "p[19,2]    0.44  1.8e-4   0.02   0.41   0.43   0.44   0.45   0.47   9811    1.0\n",
      "p[1,3]     0.05  9.8e-5 9.6e-3   0.04   0.05   0.05   0.06   0.07   9542    1.0\n",
      "p[2,3]     0.05  9.1e-5 7.9e-3   0.03   0.04   0.05   0.05   0.06   7555    1.0\n",
      "p[3,3]     0.09  1.1e-4 9.6e-3   0.07   0.08   0.09   0.09   0.11   7491    1.0\n",
      "p[4,3]     0.05  8.2e-5 7.6e-3   0.04   0.05   0.05   0.06   0.07   8499    1.0\n",
      "p[5,3]     0.07  5.8e-5 5.3e-3   0.06   0.07   0.07   0.07   0.08   8568    1.0\n",
      "p[6,3]     0.03  8.2e-5 6.1e-3   0.02   0.03   0.03   0.03   0.04   5666    1.0\n",
      "p[7,3]     0.04  9.6e-5 7.8e-3   0.03   0.04   0.04   0.05   0.06   6675    1.0\n",
      "p[8,3]     0.05  7.4e-5 6.2e-3   0.03   0.04   0.05   0.05   0.06   7125    1.0\n",
      "p[9,3]     0.06  9.6e-5 9.1e-3   0.04   0.05   0.06   0.06   0.08   9005    1.0\n",
      "p[10,3]    0.05  4.6e-5 4.3e-3   0.04   0.05   0.05   0.05   0.06   8451    1.0\n",
      "p[11,3]    0.05  1.0e-4 9.2e-3   0.03   0.04   0.05   0.05   0.07   7777    1.0\n",
      "p[12,3]    0.06  9.8e-5 8.9e-3   0.04   0.05   0.06   0.06   0.08   8252    1.0\n",
      "p[13,3]    0.07  6.4e-5 6.3e-3   0.05   0.06   0.07   0.07   0.08   9513    1.0\n",
      "p[14,3]    0.07  9.6e-5 8.9e-3   0.05   0.06   0.07   0.07   0.08   8689    1.0\n",
      "p[15,3]    0.08  1.4e-4   0.01   0.06   0.07   0.08   0.09   0.11   7163    1.0\n",
      "p[16,3]    0.09  1.2e-4   0.01   0.07   0.08   0.09   0.09   0.11   7873    1.0\n",
      "p[17,3]    0.06  9.6e-5 7.9e-3   0.04   0.05   0.06   0.06   0.07   6732    1.0\n",
      "p[18,3]    0.08  1.2e-4   0.01   0.06   0.08   0.08   0.09   0.11   8079    1.0\n",
      "p[19,3]    0.11  1.5e-4   0.01   0.09    0.1   0.11   0.12   0.14   5923    1.0\n",
      "p[1,4]     0.02  6.9e-5 5.7e-3 8.9e-3   0.01   0.02   0.02   0.03   6910    1.0\n",
      "p[2,4]     0.03  7.1e-5 6.3e-3   0.02   0.02   0.03   0.03   0.04   7918    1.0\n",
      "p[3,4]     0.03  6.6e-5 5.7e-3   0.02   0.02   0.03   0.03   0.04   7434    1.0\n",
      "p[4,4]     0.01  3.6e-5 3.4e-3 5.2e-3 8.6e-3   0.01   0.01   0.02   8617    1.0\n",
      "p[5,4]     0.01  2.4e-5 2.2e-3 7.0e-3 9.3e-3   0.01   0.01   0.02   7972    1.0\n",
      "p[6,4]     0.02  5.5e-5 4.8e-3   0.01   0.02   0.02   0.02   0.03   7696    1.0\n",
      "p[7,4]     0.01  4.6e-5 4.3e-3 5.3e-3 9.1e-3   0.01   0.01   0.02   8664    1.0\n",
      "p[8,4]     0.01  3.6e-5 3.2e-3 5.9e-3 9.1e-3   0.01   0.01   0.02   7852    1.0\n",
      "p[9,4]     0.02  6.5e-5 5.5e-3   0.01   0.02   0.02   0.02   0.03   7043    1.0\n",
      "p[10,4]    0.01  2.3e-5 2.1e-3 6.6e-3 8.8e-3   0.01   0.01   0.01   7997    1.0\n",
      "p[11,4]    0.01  5.7e-5 4.7e-3 5.4e-3 9.5e-3   0.01   0.02   0.02   6678    1.0\n",
      "p[12,4]    0.02  6.0e-5 5.6e-3   0.01   0.02   0.02   0.02   0.03   8909    1.0\n",
      "p[13,4]    0.02  4.0e-5 3.5e-3   0.01   0.02   0.02   0.02   0.03   7654    1.0\n",
      "p[14,4]    0.01  4.4e-5 3.9e-3 6.3e-310.0e-3   0.01   0.02   0.02   7815    1.0\n",
      "p[15,4]    0.03  8.7e-5 7.4e-3   0.02   0.02   0.03   0.03   0.04   7200    1.0\n",
      "p[16,4]    0.01  4.4e-5 4.0e-3 5.7e-3 9.4e-3   0.01   0.01   0.02   8408    1.0\n",
      "p[17,4]    0.01  4.1e-5 3.8e-3 6.0e-3 9.7e-3   0.01   0.01   0.02   8832    1.0\n",
      "p[18,4]    0.01  5.0e-5 4.3e-3 4.5e-3 8.2e-3   0.01   0.01   0.02   7297    1.0\n",
      "p[19,4]    0.03  6.5e-5 6.0e-3   0.02   0.02   0.03   0.03   0.04   8324    1.0\n",
      "lp__     -1.4e4    0.15   5.96 -1.4e4 -1.4e4 -1.4e4 -1.4e4 -1.4e4   1494    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Oct 24 16:10:02 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1)., 'Arkansas': Inference for Stan model: anon_model_3569e5a0cd6c3c10792bde5f98dd5a51.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha[1]  20.19    1.56  17.21   2.48   8.31  15.29  26.28   71.4    121   1.02\n",
      "alpha[2]  35.62    2.89  30.65   4.58  14.67  26.62  46.19 126.98    113   1.02\n",
      "alpha[3]   2.63    0.19   2.07   0.48   1.19   2.02    3.4   8.76    123   1.02\n",
      "alpha[4]   1.42    0.07   0.96   0.31   0.77   1.17   1.78   4.23    211   1.01\n",
      "p[1,1]     0.35  3.8e-4   0.02    0.3   0.33   0.35   0.36   0.39   3322    1.0\n",
      "p[2,1]     0.36  3.6e-4   0.02   0.33   0.35   0.36   0.37    0.4   2150    1.0\n",
      "p[3,1]     0.31  3.5e-4   0.02   0.27    0.3   0.31   0.32   0.35   2868    1.0\n",
      "p[1,2]     0.59  4.5e-4   0.02   0.55   0.58   0.59   0.61   0.64   2518    1.0\n",
      "p[2,2]     0.59  3.3e-4   0.02   0.56   0.58   0.59    0.6   0.63   2655    1.0\n",
      "p[3,2]     0.61  5.6e-4   0.02   0.56   0.59   0.61   0.62   0.64   1258    1.0\n",
      "p[1,3]     0.04  1.7e-4 9.1e-3   0.03   0.03   0.04   0.05   0.06   2842    1.0\n",
      "p[2,3]     0.03  1.5e-4 6.3e-3   0.02   0.03   0.03   0.04   0.05   1727    1.0\n",
      "p[3,3]     0.05  2.8e-4 9.2e-3   0.04   0.05   0.05   0.06   0.07   1063    1.0\n",
      "p[1,4]     0.02  2.1e-4 6.7e-3   0.01   0.02   0.02   0.03   0.04   1003    1.0\n",
      "p[2,4]     0.01  6.1e-5 3.6e-3 5.2e-3 8.7e-3   0.01   0.01   0.02   3383    1.0\n",
      "p[3,4]     0.03  1.5e-4 7.2e-3   0.02   0.03   0.03   0.04   0.05   2450    1.0\n",
      "lp__      -1575    0.08   2.77  -1582  -1577  -1575  -1573  -1571   1141   1.01\n",
      "\n",
      "Samples were drawn using NUTS at Thu Oct 24 16:10:04 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1)., 'Colorado': Inference for Stan model: anon_model_3569e5a0cd6c3c10792bde5f98dd5a51.\n",
      "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
      "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
      "\n",
      "           mean se_mean     sd   2.5%    25%    50%    75%  97.5%  n_eff   Rhat\n",
      "alpha[1]  91.48    0.83   27.2  48.06  72.42  87.99 106.91 156.91   1084    1.0\n",
      "alpha[2]  83.36    0.76   24.9  44.17  65.94  79.97  97.14 144.16   1081    1.0\n",
      "alpha[3]  16.76    0.15   5.05   8.68  13.25  16.07  19.61  28.36   1093    1.0\n",
      "alpha[4]   5.79    0.05   1.73   3.01   4.58   5.56   6.77   9.89   1221    1.0\n",
      "p[1,1]     0.46  1.7e-4   0.01   0.43   0.45   0.46   0.47   0.49   6295    1.0\n",
      "p[2,1]     0.48  2.6e-4   0.02   0.44   0.46   0.48   0.49   0.52   6196    1.0\n",
      "p[3,1]     0.45  2.4e-4   0.02   0.42   0.44   0.45   0.47   0.49   6232    1.0\n",
      "p[4,1]     0.46  2.0e-4   0.02   0.42   0.44   0.46   0.47   0.49   6582    1.0\n",
      "p[5,1]     0.47  2.2e-4   0.02   0.44   0.46   0.47   0.48   0.51   6205    1.0\n",
      "p[6,1]     0.49  2.9e-4   0.02   0.45   0.48   0.49   0.51   0.53   5433    1.0\n",
      "p[7,1]     0.48  2.2e-4   0.02   0.45   0.47   0.48   0.49   0.52   6412    1.0\n",
      "p[8,1]     0.48  2.2e-4   0.02   0.45   0.47   0.48    0.5   0.52   6588    1.0\n",
      "p[9,1]     0.42  2.4e-4   0.02   0.39   0.41   0.42   0.44   0.46   4694    1.0\n",
      "p[10,1]    0.46  2.1e-4   0.02   0.43   0.45   0.46   0.47   0.49   5602    1.0\n",
      "p[11,1]    0.45  2.0e-4   0.02   0.42   0.44   0.45   0.46   0.49   7249    1.0\n",
      "p[12,1]    0.46  2.6e-4   0.02   0.42   0.45   0.46   0.47    0.5   5673    1.0\n",
      "p[13,1]    0.42  2.5e-4   0.02   0.38    0.4   0.42   0.43   0.45   5648    1.0\n",
      "p[14,1]    0.49  2.0e-4   0.02   0.46   0.48   0.49    0.5   0.52   6991    1.0\n",
      "p[15,1]    0.47  2.4e-4   0.02   0.44   0.46   0.47   0.49   0.51   5868    1.0\n",
      "p[16,1]    0.47  1.8e-4   0.02   0.44   0.46   0.47   0.48    0.5   7456    1.0\n",
      "p[1,2]     0.45  1.7e-4   0.01   0.42   0.44   0.45   0.45   0.47   6198    1.0\n",
      "p[2,2]     0.42  2.6e-4   0.02   0.38   0.41   0.42   0.43   0.46   6189    1.0\n",
      "p[3,2]     0.44  2.6e-4   0.02    0.4   0.43   0.44   0.45   0.48   5642    1.0\n",
      "p[4,2]     0.42  2.0e-4   0.02   0.39   0.41   0.42   0.43   0.45   7038    1.0\n",
      "p[5,2]      0.4  2.3e-4   0.02   0.37   0.39    0.4   0.41   0.43   5387    1.0\n",
      "p[6,2]      0.4  2.7e-4   0.02   0.36   0.39    0.4   0.42   0.44   5769    1.0\n",
      "p[7,2]     0.38  2.4e-4   0.02   0.35   0.37   0.38   0.39   0.42   5432    1.0\n",
      "p[8,2]     0.43  2.1e-4   0.02   0.39   0.41   0.43   0.44   0.46   6580    1.0\n",
      "p[9,2]     0.42  2.1e-4   0.02   0.39   0.41   0.42   0.44   0.46   5482    1.0\n",
      "p[10,2]    0.44  2.1e-4   0.02   0.41   0.43   0.44   0.45   0.47   5598    1.0\n",
      "p[11,2]    0.43  2.0e-4   0.02    0.4   0.42   0.43   0.44   0.46   6949    1.0\n",
      "p[12,2]    0.39  2.6e-4   0.02   0.35   0.38   0.39    0.4   0.43   5667    1.0\n",
      "p[13,2]    0.44  2.3e-4   0.02    0.4   0.42   0.44   0.45   0.47   6264    1.0\n",
      "p[14,2]    0.44  1.9e-4   0.02   0.41   0.43   0.44   0.45   0.47   7853    1.0\n",
      "p[15,2]    0.42  2.2e-4   0.02   0.39   0.41   0.42   0.43   0.46   6664    1.0\n",
      "p[16,2]    0.43  1.8e-4   0.02    0.4   0.42   0.43   0.44   0.46   7120    1.0\n",
      "p[1,3]     0.06  8.4e-5 6.4e-3   0.04   0.05   0.06   0.06   0.07   5714    1.0\n",
      "p[2,3]     0.08  1.4e-4   0.01   0.06   0.07   0.08   0.09    0.1   5407    1.0\n",
      "p[3,3]     0.06  1.3e-4 9.8e-3   0.05   0.06   0.06   0.07   0.08   5562    1.0\n",
      "p[4,3]     0.08  1.1e-4 9.1e-3   0.07   0.08   0.08   0.09    0.1   6478    1.0\n",
      "p[5,3]      0.1  1.4e-4   0.01   0.08   0.09    0.1   0.11   0.12   5788    1.0\n",
      "p[6,3]     0.08  1.5e-4   0.01   0.06   0.07   0.08   0.08    0.1   5385    1.0\n",
      "p[7,3]      0.1  1.4e-4   0.01   0.08    0.1    0.1   0.11   0.13   6225    1.0\n",
      "p[8,3]     0.07  1.2e-4 9.0e-3   0.05   0.06   0.07   0.07   0.09   6111    1.0\n",
      "p[9,3]     0.12  1.5e-4   0.01    0.1   0.11   0.12   0.13   0.14   4815    1.0\n",
      "p[10,3]    0.08  1.1e-4 8.6e-3   0.06   0.07   0.08   0.09    0.1   6250    1.0\n",
      "p[11,3]     0.1  1.4e-4   0.01   0.08   0.09    0.1    0.1   0.12   5676    1.0\n",
      "p[12,3]    0.12  1.8e-4   0.01   0.09   0.11   0.12   0.13   0.15   5017    1.0\n",
      "p[13,3]    0.12  1.8e-4   0.01    0.1   0.12   0.12   0.13   0.15   4839    1.0\n",
      "p[14,3]    0.05  1.2e-4 7.9e-3   0.04   0.05   0.05   0.06   0.07   4455    1.0\n",
      "p[15,3]    0.08  1.3e-4 9.9e-3   0.06   0.07   0.08   0.09    0.1   6197    1.0\n",
      "p[16,3]    0.08  9.9e-5 8.3e-3   0.06   0.07   0.08   0.08    0.1   7116    1.0\n",
      "p[1,4]     0.04  6.4e-5 5.2e-3   0.03   0.04   0.04   0.04   0.05   6510    1.0\n",
      "p[2,4]     0.02  7.8e-5 6.2e-3   0.01   0.02   0.02   0.03   0.04   6443    1.0\n",
      "p[3,4]     0.04  1.1e-4 7.9e-3   0.03   0.04   0.04   0.05   0.06   5271    1.0\n",
      "p[4,4]     0.04  9.1e-5 6.5e-3   0.03   0.03   0.04   0.04   0.05   5110    1.0\n",
      "p[5,4]     0.03  7.5e-5 6.0e-3   0.02   0.03   0.03   0.03   0.04   6289    1.0\n",
      "p[6,4]     0.03  9.0e-5 7.1e-3   0.02   0.03   0.03   0.03   0.05   6297    1.0\n",
      "p[7,4]     0.03  8.4e-5 6.6e-3   0.02   0.03   0.03   0.04   0.05   6274    1.0\n",
      "p[8,4]     0.02  7.1e-5 5.2e-3   0.01   0.02   0.02   0.03   0.03   5388    1.0\n",
      "p[9,4]     0.03  6.5e-5 5.4e-3   0.02   0.03   0.03   0.03   0.04   6942    1.0\n",
      "p[10,4]    0.02  6.0e-5 4.6e-3   0.02   0.02   0.02   0.03   0.03   6036    1.0\n",
      "p[11,4]    0.02  6.6e-5 5.2e-3   0.01   0.02   0.02   0.02   0.03   6265    1.0\n",
      "p[12,4]    0.03  8.4e-5 6.7e-3   0.02   0.03   0.03   0.04   0.05   6366    1.0\n",
      "p[13,4]    0.02  7.6e-5 5.6e-3   0.01   0.02   0.02   0.03   0.04   5355    1.0\n",
      "p[14,4]    0.02  6.0e-5 5.2e-3   0.01   0.02   0.02   0.03   0.03   7417    1.0\n",
      "p[15,4]    0.02  7.2e-5 5.7e-3   0.01   0.02   0.02   0.03   0.04   6163    1.0\n",
      "p[16,4]    0.02  5.5e-5 4.4e-3   0.01   0.02   0.02   0.03   0.03   6478    1.0\n",
      "lp__     -1.1e4    0.14   5.52 -1.1e4 -1.1e4 -1.1e4 -1.1e4 -1.1e4   1528    1.0\n",
      "\n",
      "Samples were drawn using NUTS at Thu Oct 24 16:10:19 2019.\n",
      "For each parameter, n_eff is a crude measure of effective sample size,\n",
      "and Rhat is the potential scale reduction factor on split chains (at \n",
      "convergence, Rhat=1).}\n"
     ]
    }
   ],
   "source": [
    "poll_results = {}\n",
    "\n",
    "for state in states:\n",
    "    survey_data = {\n",
    "        'N': 4,\n",
    "        'S': len(survey_results[state]),\n",
    "        'polls': survey_results[state]\n",
    "    }\n",
    "    poll_results[state] = stan_model.sampling(data=survey_data)\n",
    "    \n",
    "print(poll_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_survey_data(state):\n",
    "    survey_data = {\n",
    "        'N': 4,\n",
    "        'S': len(survey_results[state]),\n",
    "        'polls': survey_results[state]\n",
    "    }\n",
    "    return survey_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation time\n",
    "\n",
    "Use the posterior samples to predict the outcome of the presidential elections.\n",
    "\n",
    "* Predict the probability that each candidate **will win each state**.\n",
    "   * Use the posterior $\\alpha$ samples to generate posterior predictive samples for $p$ — the proportion of votes each candidate would get in each state in an election.\n",
    "   * Use these $p$ samples to estimate the probability that each candidate will win each state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_ps(results):\n",
    "\n",
    "    samples = results.extract()\n",
    "\n",
    "    # Make a new array with same dimensions as alpha\n",
    "    p_predicted = np.empty(samples['alpha'].shape)\n",
    "    # Generate one p sample for each alpha sample\n",
    "    for i in range(samples['alpha'].shape[0]):\n",
    "        p_predicted[i] = stats.dirichlet(samples['alpha'][i]).rvs()\n",
    "    \n",
    "    return p_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chances of winning  Alabama\n",
      "[0.25164425 0.24773114 0.25105299 0.24957162]\n",
      "Chances of winning  Alaska\n",
      "[0.40733986 0.42327298 0.1163932  0.05299395]\n",
      "Chances of winning  Arizona\n",
      "[0.45215685 0.46796059 0.06194643 0.01793613]\n",
      "Chances of winning  Arkansas\n",
      "[0.33307032 0.58605135 0.04916752 0.03171081]\n",
      "Chances of winning  Colorado\n",
      "[0.46351758 0.42250452 0.08466454 0.02931337]\n"
     ]
    }
   ],
   "source": [
    "for state in states:\n",
    "    results = poll_results[state]\n",
    "    p_pred = get_ps(results)\n",
    "    print(\"Chances of winning \", state)\n",
    "    print(np.mean(p_pred, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Predict the probability that each candidate **will win the presidential election.**\n",
    "   * Use the posterior predictive probability that each candidate will win each state to generate samples over the total number Electoral College votes each candidate would get in an election.\n",
    "   * Use the total number of votes to generate samples over who would win the election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:97 of 4000 iterations ended with a divergence (2.42 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.8 to remove the divergences.\n"
     ]
    }
   ],
   "source": [
    "ps_states = {}\n",
    "for state in states:     #retrieveing sample probabilities for each state\n",
    "\n",
    "    state_data = get_survey_data(state)\n",
    "    results = stan_model.sampling(data=state_data)\n",
    "    ps_states[state] = get_ps(results)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate 4000 different elections:\n",
    "electoral_votes = {\n",
    "    'Alabama': 9,\n",
    "    'Alaska': 3,\n",
    "    'Arizona': 11,\n",
    "    'Arkansas': 6,\n",
    "    'Colorado': 9,\n",
    "}\n",
    "election_wins = [0, 0, 0, 0]\n",
    "\n",
    "for election in range(4000):   #4k fake elections\n",
    "    \n",
    "    votes = [0, 0, 0, 0]\n",
    "    \n",
    "    for state in states:       #simulating each state's results\n",
    "        p = ps_states[state][election] \n",
    "        winner = np.random.choice(range(0,4),p=p)\n",
    "\n",
    "        votes[winner] += electoral_votes[state]\n",
    "\n",
    "    election_wins[np.argmax(votes)] += 1   #most votes wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chances of winning the election:\n",
      "Total:  [1739, 1985, 209, 67]\n",
      "Percentage of winning:  [0.43475 0.49625 0.05225 0.01675]\n"
     ]
    }
   ],
   "source": [
    "print(\"Chances of winning the election:\")\n",
    "\n",
    "print(\"Total: \", election_wins)\n",
    "print(\"Percentage of winning: \", election_wins/np.sum(election_wins))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
